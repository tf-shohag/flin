# üöÄ Flin Performance Optimization Guide: Building NATS-Level Speed

A comprehensive guide to achieving extreme performance in Go, inspired by NATS and other high-performance systems.

## üìã Table of Contents

1. [Memory Management](#memory-management)
2. [Concurrency Patterns](#concurrency-patterns)
3. [Network Optimization](#network-optimization)
4. [I/O Performance](#io-performance)
5. [Data Structures](#data-structures)
6. [Compiler & Runtime Optimization](#compiler--runtime-optimization)
7. [Profiling & Benchmarking](#profiling--benchmarking)
8. [Architecture Patterns](#architecture-patterns)

---

## üß† Memory Management

### Zero-Allocation Strategies

**Buffer Pooling with sync.Pool**
```go
var bufferPool = sync.Pool{
    New: func() interface{} {
        return make([]byte, 4096)
    },
}

// Reuse buffers instead of allocating
buf := bufferPool.Get().([]byte)
defer bufferPool.Put(buf)
```

**Pre-allocate Slices**
```go
// Bad: Multiple allocations
messages := []Message{}
for i := 0; i < 1000; i++ {
    messages = append(messages, msg)
}

// Good: Single allocation
messages := make([]Message, 0, 1000)
for i := 0; i < 1000; i++ {
    messages = append(messages, msg)
}
```

**String to Byte Conversion (Zero-Copy)**
```go
import "unsafe"

// Use sparingly and carefully
func stringToBytes(s string) []byte {
    return unsafe.Slice(unsafe.StringData(s), len(s))
}

func bytesToString(b []byte) string {
    return unsafe.String(unsafe.SliceData(b), len(b))
}
```

**Avoid Interface Allocations**
```go
// Bad: Allocates on heap
func process(val interface{}) {}

// Good: Use generics (Go 1.18+)
func process[T any](val T) {}
```

---

## ‚ö° Concurrency Patterns

### Lock-Free Data Structures

**Use atomic operations instead of mutexes where possible**
```go
import "sync/atomic"

type Counter struct {
    value atomic.Int64
}

func (c *Counter) Inc() {
    c.value.Add(1)
}

func (c *Counter) Get() int64 {
    return c.value.Load()
}
```

### Efficient Channel Usage

**Buffered Channels for Throughput**
```go
// Bad: Unbuffered causes blocking
ch := make(chan Message)

// Good: Buffer reduces contention
ch := make(chan Message, 1000)
```

**Fan-Out Pattern for Parallel Processing**
```go
func fanOut(input <-chan Task, workers int) []<-chan Result {
    outputs := make([]<-chan Result, workers)
    
    for i := 0; i < workers; i++ {
        output := make(chan Result, 100)
        outputs[i] = output
        
        go func() {
            for task := range input {
                output <- processTask(task)
            }
            close(output)
        }()
    }
    
    return outputs
}
```

### Worker Pool Pattern

**Reuse goroutines instead of spawning millions**
```go
type WorkerPool struct {
    tasks chan Task
    wg    sync.WaitGroup
}

func NewWorkerPool(workers int) *WorkerPool {
    pool := &WorkerPool{
        tasks: make(chan Task, 1000),
    }
    
    for i := 0; i < workers; i++ {
        pool.wg.Add(1)
        go pool.worker()
    }
    
    return pool
}

func (p *WorkerPool) worker() {
    defer p.wg.Done()
    for task := range p.tasks {
        task.Execute()
    }
}

---

## üåê Network Optimization

### gRPC Performance Guide

## üéØ Current Performance (Optimized)

‚úÖ **4 workers (optimal): 103K SET ops/sec, 787K GET ops/sec**
‚úÖ **Mixed workload: 140K ops/sec (70% GET, 30% SET)**
‚úÖ **Delete: 165K ops/sec**

**Configuration:** 4 workers, 512MB cache, async writesning

**Connection Pooling**
```go
// Configure gRPC with performance options
conn, err := grpc.Dial(
    address,
    grpc.WithTransportCredentials(insecure.NewCredentials()),
    grpc.WithInitialWindowSize(1 << 20),      // 1MB
    grpc.WithInitialConnWindowSize(1 << 20),  // 1MB
    grpc.WithKeepaliveParams(keepalive.ClientParameters{
        Time:                10 * time.Second,
        Timeout:             3 * time.Second,
        PermitWithoutStream: true,
    }),
    grpc.WithDefaultCallOptions(
        grpc.MaxCallRecvMsgSize(10 * 1024 * 1024), // 10MB
        grpc.MaxCallSendMsgSize(10 * 1024 * 1024),
    ),
)
```

**Streaming for High Throughput**
```go
// Use bidirectional streaming for continuous data flow
stream, err := client.StreamData(ctx)

// Pipeline: read and write concurrently
go func() {
    for {
        msg, err := stream.Recv()
        if err != nil {
            return
        }
        process(msg)
    }
}()

for data := range dataChannel {
    stream.Send(data)
}
```

### TCP Tuning

**Optimize socket options**
```go
import "golang.org/x/sys/unix"

func setSocketOptions(fd int) error {
    // Disable Nagle's algorithm for low latency
    unix.SetsockoptInt(fd, unix.IPPROTO_TCP, unix.TCP_NODELAY, 1)
    
    // Enable TCP quickack
    unix.SetsockoptInt(fd, unix.IPPROTO_TCP, unix.TCP_QUICKACK, 1)
    
    // Set buffer sizes
    unix.SetsockoptInt(fd, unix.SOL_SOCKET, unix.SO_RCVBUF, 4194304) // 4MB
    unix.SetsockoptInt(fd, unix.SOL_SOCKET, unix.SO_SNDBUF, 4194304)
    
    return nil
}
```

---

## üíæ I/O Performance

### BadgerDB Optimization

**Batch Writes**
```go
// Bad: Individual writes
for _, kv := range data {
    db.Update(func(txn *badger.Txn) error {
        return txn.Set(kv.Key, kv.Value)
    })
}

// Good: Batch transaction
wb := db.NewWriteBatch()
defer wb.Cancel()

for _, kv := range data {
    wb.Set(kv.Key, kv.Value)
}
wb.Flush()
```

**Configure for Performance**
```go
opts := badger.DefaultOptions("/tmp/badger").
    WithNumVersionsToKeep(1).
    WithNumMemtables(3).
    WithMemTableSize(64 << 20).        // 64MB
    WithValueLogFileSize(256 << 20).   // 256MB
    WithMaxTableSize(16 << 20).        // 16MB
    WithNumLevelZeroTables(5).
    WithNumLevelZeroTablesStall(10).
    WithValueThreshold(1024).          // Store in vlog if > 1KB
    WithNumCompactors(4).
    WithCompactL0OnClose(false)

db, err := badger.Open(opts)
```

### Memory-Mapped I/O

**Use for fast reads**
```go
import "golang.org/x/exp/mmap"

reader, err := mmap.Open(filename)
defer reader.Close()

// Direct memory access, zero-copy
data := reader.At(offset, length)
```

---

## üóÑÔ∏è Data Structures

### High-Performance Maps

**Use sync.Map for concurrent access**
```go
var cache sync.Map

// Lock-free reads and writes
cache.Store(key, value)
val, ok := cache.Load(key)
```

**Or use sharded maps for better performance**
```go
type ShardedMap struct {
    shards []*shard
    count  int
}

type shard struct {
    sync.RWMutex
    items map[string]interface{}
}

func (m *ShardedMap) getShard(key string) *shard {
    hash := fnv32(key)
    return m.shards[hash%uint32(m.count)]
}

func (m *ShardedMap) Set(key string, val interface{}) {
    shard := m.getShard(key)
    shard.Lock()
    shard.items[key] = val
    shard.Unlock()
}
```

### Ring Buffers

**Lock-free queue for single producer/consumer**
```go
type RingBuffer struct {
    buffer  []interface{}
    head    atomic.Uint64
    tail    atomic.Uint64
    size    uint64
}

func (rb *RingBuffer) Push(item interface{}) bool {
    head := rb.head.Load()
    next := (head + 1) % rb.size
    
    if next == rb.tail.Load() {
        return false // Full
    }
    
    rb.buffer[head] = item
    rb.head.Store(next)
    return true
}
```

---

## üîß Compiler & Runtime Optimization

### Build Flags

**Compile for maximum performance**
```bash
# Disable bounds checking (use carefully!)
go build -gcflags="-B"

# Inline optimization
go build -gcflags="-l=4"

# Link-time optimization
go build -ldflags="-s -w"

# Profile-guided optimization (PGO)
go build -pgo=default.pgo
```

### GOMAXPROCS Tuning

**Set based on workload**
```go
import "runtime"

// For CPU-bound work: use all cores
runtime.GOMAXPROCS(runtime.NumCPU())

// For I/O-bound work: may benefit from more
runtime.GOMAXPROCS(runtime.NumCPU() * 2)
```

### Escape Analysis

**Keep allocations on stack**
```bash
# Check escape analysis
go build -gcflags="-m"
```

```go
// Bad: Escapes to heap
func bad() *int {
    x := 42
    return &x  // Escapes!
}

// Good: Stays on stack
func good() int {
    x := 42
    return x
}
```

---

## üìä Profiling & Benchmarking

### pprof Integration

**Add profiling endpoints**
```go
import _ "net/http/pprof"

go func() {
    http.ListenAndServe("localhost:6060", nil)
}()
```

**Profile your application**
```bash
# CPU profile
go tool pprof http://localhost:6060/debug/pprof/profile?seconds=30

# Memory profile
go tool pprof http://localhost:6060/debug/pprof/heap

# Goroutine profile
go tool pprof http://localhost:6060/debug/pprof/goroutine

# Block profile (mutex contention)
go tool pprof http://localhost:6060/debug/pprof/block
```

### Benchmarking

**Write comprehensive benchmarks**
```go
func BenchmarkMessageProcess(b *testing.B) {
    // Setup
    msg := generateMessage()
    
    b.ResetTimer()
    b.ReportAllocs()
    
    for i := 0; i < b.N; i++ {
        processMessage(msg)
    }
}

// Run with different scenarios
func BenchmarkWithParallel(b *testing.B) {
    b.RunParallel(func(pb *testing.PB) {
        for pb.Next() {
            processMessage(msg)
        }
    })
}
```

---

## üèóÔ∏è Architecture Patterns

### NATS-Style Architecture

**Single-threaded event loop per connection**
```go
type Connection struct {
    conn   net.Conn
    parser *Parser
    outQ   chan []byte
}

func (c *Connection) readLoop() {
    buf := make([]byte, 32768)
    for {
        n, err := c.conn.Read(buf)
        if err != nil {
            return
        }
        
        // Parse and route in same goroutine
        c.parser.Parse(buf[:n])
    }
}

func (c *Connection) writeLoop() {
    for msg := range c.outQ {
        c.conn.Write(msg)
    }
}
```

### Fast Path Optimization

**Optimize the common case**
```go
// Fast path: no allocation for small messages
func (f *Flin) HandleMessage(msg []byte) {
    if len(msg) < 4096 {
        f.processFast(msg)  // Stack-allocated buffer
        return
    }
    
    // Slow path: large messages
    f.processSlow(msg)
}
```

### Zero-Copy Forwarding

**Avoid data copying in message routing**
```go
type Message struct {
    buf    []byte  // Reference to original buffer
    offset int     // Start position
    length int     // Message length
}

// Just pass references, don't copy data
func (r *Router) Forward(msg *Message, subscribers []Subscriber) {
    for _, sub := range subscribers {
        sub.Send(msg) // Send reference, not copy
    }
}
```

---

## üéØ Flin-Specific Recommendations

### 1. **Message Protocol Design**
- Use binary protocol (not JSON) for internal communication
- Fixed-size headers for fast parsing
- Length-prefixed messages to avoid scanning

### 2. **Connection Handling**
- One goroutine per connection for reads
- One goroutine per connection for writes
- Share a global routing goroutine

### 3. **Clustering**
- Use Raft (via hashicorp/raft) for consensus
- Implement fast follower reads
- Batch replication messages

### 4. **Memory Management**
- Pre-allocate message buffers
- Use object pools for common types
- Implement your own slab allocator for fixed-size objects

### 5. **Monitoring**
- Use atomic counters (no locks)
- Batch metric updates
- Export via Prometheus pull model

---

## üìà Performance Targets

Based on NATS benchmarks, aim for:

- **Latency**: < 100Œºs p99 for pub/sub
- **Throughput**: > 10M msgs/sec per node
- **Memory**: < 10KB per connection
- **CPU**: < 50% at 1M msgs/sec

---

## üîó Additional Resources

- [NATS Server Source](https://github.com/nats-io/nats-server)
- [Go Performance Wiki](https://github.com/golang/go/wiki/Performance)
- [Dave Cheney's High Performance Go Workshop](https://dave.cheney.net/high-performance-go-workshop/gopherchina-2019.html)
- [BadgerDB Best Practices](https://dgraph.io/docs/badger/get-started/)

---

## ‚ö° Quick Wins Checklist

- [ ] Enable PGO (Profile-Guided Optimization)
- [ ] Use `sync.Pool` for frequently allocated objects
- [ ] Pre-allocate slices with known capacity
- [ ] Replace `interface{}` with generics
- [ ] Use buffered channels appropriately
- [ ] Implement connection pooling
- [ ] Batch database writes
- [ ] Use streaming for bulk operations
- [ ] Profile regularly with pprof
- [ ] Write benchmarks for critical paths
- [ ] Tune GOMAXPROCS for your workload
- [ ] Enable TCP_NODELAY for low latency
- [ ] Use atomic operations instead of mutexes
- [ ] Implement worker pools for concurrent tasks
- [ ] Monitor memory allocations with `-benchmem`

---

**Remember**: Always measure before and after optimization. Premature optimization is the root of all evil, but informed optimization based on profiling data is the path to NATS-level performance! üöÄ